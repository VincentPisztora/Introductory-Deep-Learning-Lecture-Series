# Introductory Deep Learning Lecture Series

This directory contains lecture notes from a short series of lectures I delivered on the basics of deep learning. The lectures introduce the general framework of statistical learning as loss minimization, and introduce deep learning as a special case. First, I provide a brief history of deep learning. Then, I cover the general ingredients of any deep learning problem: (1) loss function (2) optimizer, and (3) architecture. The basic examples of each that are covered:

1. Losses: MSE, Cross-Entropy
2. Optimizers: Stochastic Gradient Descent, Momentum, RMSProp, Adam
3. Architectures: ResNet (Dense, 2D Convolution, Residual, Batch Normalization, and Dropout layers)
