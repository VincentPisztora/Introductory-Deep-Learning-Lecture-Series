# Introductory Deep Learning Lecture Series

This directory contains lecture notes from a short series of lectures I delivered on the basics of deep learning. The lectures introduce the general framework of statistical learning as loss minimization, and introduce deep learning as a special case. First, I provide a brief history of deep learning. Then, I cover the general ingredients of any deep learning problem: (1) loss functions, (2) optimization, and (3) architecture. The basic examples of each that are covered:

1. Losses: MSE, Cross-Entropy
2. Optimization: Backpropagation, Stochastic Gradient Descent, Momentum, RMSProp, Adam
3. Architectures: ResNet (Dense, 2D Convolution, Pooling, Residual, Batch Normalization, and Dropout layers)
